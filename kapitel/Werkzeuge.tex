\listoftodos


\chapter{Werkzeuge}
\label{chap:Werkzeuge}
Dieses Kapitel erläutert die erstellten Werkzeuge im Detail. Die Werkzeuge bauen auf einander auf und werden der Reihe nach erläutert. 

	\section{ConvolutionalSecondCriterionAutoenocder}
	\label{sec:SecondCriterionAutoenocder}
	Der SCAE erweitert einen ConvolutionalAutoencoder um ein weiteres Kriterium. Es gibt also zusätzlich zu der Rekonstruktion des Autoencoders einen weiteren Ausgang. Der zweite Ausgang kann wie jeder Ausgang für eine Binärklassifikation, für eine Multiklassifikation, für eine Regression oder jede andere belibige Aufgabe genutzt werden. In Abbildung \ref{img:SchemaSCAE} ist der schematische Aufbau des SCAE abgebildet. Die Schichten des zweiten Kriterium,s werden an das Code-Schicht des NN angehängt. Es können belibig viele Schichten genutzt werden. Die Verlustfunktion des NN besteht aus der Summe der einzelnen Verlustfunktionen und einer Gewichtung. Sie lautet im Detail: 
	\begin{align}
	loss = weight1 * loss\_autoencoder + weight2 * loss\_secondcriterion
	\end{align}
	Die Gewichtung der Verlustfunktionen kann dem SCAE per Konstruktor-Argument übergeben werden. Das Werkzeug ist als Python-Module implementiert. Dabei implementiert die Klasse ConvolutionalSecondCriterionAutoenocder den ConvolutionalAutoenocder aus Psipy. In Abbildung \ref{img:KlassendiagrammCSCAE} ist das Klassendiagramm des SCAE dargestellt.
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.5\textwidth, center]{bilder/Schema_Autoencoders/Schema_SCAE.png}
		\caption[Schema SecondCriterionAutoenocder]{Schema SecondCriterionAutoenocder}
		\label{img:SchemaSCAE}
	\end{figure}  
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.5\textwidth, center]{bilder/Klassendiagramme/Klassendiagramm_CSCAE.png}
		\caption[Klassendiagramm ConvolutionalSecondCriterionAutoencoder]{Klassendiagramm ConvolutionalSecondCriterionAutoencoder}
	    \label{img:KlassendiagrammCSCAE}
	\end{figure}  
    Über den Konstruktor können alle Argumente welche zum Erstellen des Models notwendig sind per doppeltes Sternchen Wörterburch Argument (**kwargs) an die Klasse übergeben werden. Diese Technik erlaubt es eine mit Schlüsselwörtern versehene Argumentliste variabler Länge zu übergeben.  Die Argumentlisten  werden beinahe in allen Methode zum Einsatz gebracht. Sie werden insbesodnere genutzt um Argumente an die zugehörigen Keras-Methoden zu übergeben. Die Namensgebung der Methode orientiert sich dabei an Keras. So wird z.B. in dem Methodenaufruf $fit(..)$ unter anderem auch die Keras-Methode  $fit(..)$ aufgerufen. Um einen SCAE zu trainieren ist es notwendig einen Instanz zu erzeugen, die Methode $pretrain(..)$ aufzurufen und ihn Anschliesend mit der Methode $fit(..)$ zu trainieren.
    In dem Methodenaufruf $pretrain(..)$ wird das Modell erstellt und Schichtenweise vortrainiert. Das eigentliche Training erfolgt in der Methode $fit(..)$. Alternativ können auch die zugehörigen Generatorenklassen aufgerufen werden.    
    
    In Listing \ref{lst:BspErstellungConvolutionalSecondCriterionAutoenocder} ist beispielhafte dargestellt wie ein SCAE erstellt wird. In den ersten 10 Zeilen wird die Architektur erstellt. Ab Zeile 12 wird eine Instanz eines  CSCA mittels Arguemntenliste erstellt. Zu beachten ist, dass hier keine Decoder-Architektur übergeben wird. Wenn keine Decoder-Architektur bereritgestellt wird wird sie beim Erstellen des eigentlichen Modells aus der Encoder-Architketur abgeleitet.
	\begin{lstlisting}[language=python,caption=Beispiel Erstellung ConvolutionalSecondCriterionAutoenocder in Python, label=lst:BspErstellungConvolutionalSecondCriterionAutoenocder]
	encoder_topology = [("Conv2D", {"filters": 8, "kernel_size": (3, 3)}),
	("Conv2D", {"filters": 8, "kernel_size": (3, 3)}),
	('MaxPooling2D', {"pool_size": (2, 2)}),
	("Conv2D", {"filters": 16, "kernel_size": (3, 3)}),
	("MaxPooling2D", {"pool_size": (2, 2)}),
	("Conv2D", {"filters": 16, "kernel_size": (3, 3)}),
	("Flatten", {}),
	("Dense", {"units": 16})]

	second_criterion_topology = [("Dense", {"units": num_classes}) ]

	csca = ConvolutionalSecondCriterionAutoencoder(
	input_shape=(28, 28, 1),	
	code_dimensions=3, 
	encoder_topology=encoder_topology,
	second_criterion_topology=second_criterion_topology,
	hidden_layer_kwargs = {'activation': 'relu'},
	output_layer_kwargs = {'activation': 'sigmoid'},
	second_criterion_hidden_layer_kwargs = {'activation': 'relu'},
	second_criterion_output_layer_kwargs = {'activation': 'softmax'},
	second_criterion_loss = 'categorical_crossentropy',
	loss_weights=[8., 1.],
	second_criterion_metrics = {'second_criterion':'accuracy'},
	code_layer_kwargs=dict(),        
	)
	\end{lstlisting}
Listing  \ref{lst:BspPretrainConvolutionalSecondCriterionAutoenocder}  zeigt den Aufruf der Methode Pretrain. Der Aufruf führt zu einem Schichtenweise-Trainieren des Netzwerkes mit den Daten x\_train bei 20 Epochen und einer Stapelgröße von 64. 
\begin{lstlisting}[language=python,caption=Beispielaufruf Pretrain  in Python, label=lst:BspPretrainConvolutionalSecondCriterionAutoenocder]
csca.pretrain(x_train,epochs = 20, batch_size = 64)
\end{lstlisting}

Der Methodenaufruf $fit(..)$ funktioniert wie der $fit(..)$-Aufruf in Keras.  In Zeile drei des Listing  \ref{lst:BspPretrainConvolutionalSecondCriterionAutoenocder}  ist zu erkennen, dass die Zielgrößen der verschiedenen Ausgänge einfach als Python-Wörterbuch übergeben werden können.
\begin{lstlisting}[language=python,caption=Beispielaufruf Fit  in Python, label=lst:BspFitConvolutionalSecondCriterionAutoenocder]
history = csca.fit(
x_train,
{"decoder": x_train, "second_criterion": y_train}, 
epochs=200,
batch_size = 64,
validation_data=(x_test,{"decoder": x_test, "second_criterion": y_test}))
\end{lstlisting}

	\section{TransferSecondCriterionAutoenocder}
	\label{sec:TransferSecondCriterionAutoenocder}
		
	Ein TransferSecondCriterionAutoenocder basiert auf einem SCAE. Das zweite Kriterium wird durch ein neues Kriterium ersetzt. Zum Beispiel kann ein SCAE eine Objekterkennung durchführen. Bei einem TransferSecondCriterionAutoenocder wird die Objekerkennung durch eine Klassifiaktaionsaufgabe ersetzt. Abbildung \ref{img:SchemaTSCAE} zeigt den Aufbau des TSCAE.
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.5\textwidth, center]{bilder/Schema_Autoencoders/Schema_TSCAE.png}
		\caption[Schema TransferSecondCriterionAutoenocder]{Schema TransferSecondCriterionAutoenocder}
		\label{img:SchemaTSCAE}
	\end{figure}  
	Im Klassendiagramm \ref{img:KlassendiagrammTransferSecondCriterionAutoenocder} für den TSCAE ist zu sehen. Das besondere ist, dass ein SCAE als Konstruktorargument übergeben wird. Die Einstellungen für den ConvolutionalAutoencoder werden aus diesem Model kopiert. Zusätlich müssen nur noch die Einstellungen für das zweite Kriterium übergeben werden. 	
	Listing \ref{lst:BspTransferSecondCriterionAutoenocder} zeigt einen Beispielhaft die Anwendung dieses Werkzeuges. Im Vergleich zu dem sCAE ist die Anwendung schon deutlich einfacher. Es gibt weniger Hyperparameter zum Beachten. Da das Modell auf einem trainierten SCAE basiert ist ist kein $pretrain(..)$ mehr notwenig. Die Methode $fit(..)$ wird auf die selbe Weisse wie bei SCAE angwendet.	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.5\textwidth, center]{bilder/Klassendiagramme/Klassendiagramm_TLCSCAE.png}
		\caption[Klassendiagramm TransferSecondCriterionAutoenocder]{Klassendiagramm TransferSecondCriterionAutoenocder}
		\label{img:KlassendiagrammTransferSecondCriterionAutoenocder}
	\end{figure}  

	\begin{lstlisting}[language=python,caption=Beispiel TransferSecondCriterionAutoenocder in Python, label=lst:BspTransferSecondCriterionAutoenocder]
	tscm = TransferLearningConvolutionalSecondCriterionAutoencoder(csc_autoencoder,
	second_criterion_topology=second_criterion_topology,
	second_criterion_loss = 'binary_crossentropy',                                                                                                   
	second_criterion_hidden_layer_kwargs = {'activation': 'relu'},
	second_criterion_output_layer_kwargs = {'activation': 'sigmoid'}, 
	loss_weights=[1, 0.01],
	freeze_encoder_layers = 2
	,freeze_decoder_layers =2)
	
	
	history = tscm.fit(
	x_train,
	{"decoder": x_train, "second_criterion": y_train}, 
	epochs=1,
	batch_size = 128,
	validation_data=(x_test,{"decoder": x_test, "second_criterion": y_test}))
	)
	\end{lstlisting}
			
	\section{AutoTransferSecondCriterionAutoenocder}
	\label{sec:AutoTransferSecondCriterionAutoenocder}


	\begin{figure}[h]
		\centering
		\includegraphics[width=0.5\textwidth, center]{bilder/Klassendiagramme/Klassendiagramm_AutoTLCSCAE.png}
		\caption[Klassendiagramm AutoTransferSecondCriterionAutoenocder]{Klassendiagramm AutoTransferSecondCriterionAutoenocder}
		\label{img:KlassendiagrammAutoTransferSecondCriterionAutoenocder}
	\end{figure}  

	\begin{lstlisting}[language=python,caption=Beispiel AutoTransferSecondCriterionAutoenocder in Python, label=lst:BspAutoTransferSecondCriterionAutoenocder]
	tscm = AutoTransferConvolutionalSecondCriterionAutoencoder(max_deep_freeze=2,
	path_to_model = path_to_base_model,            
	second_criterion_topology=second_criterion_topology,
	second_criterion_loss = 'categorical_crossentropy',                                                                                                   
	second_criterion_hidden_layer_kwargs = {'activation': 'relu'},
	second_criterion_output_layer_kwargs = {'activation': 'softmax'},
	second_criterion_metrics = {'second_criterion':'accuracy'}
	)
	
	tscm.set_generators(train_datagenerator,test_datagenerator)
	
	
	best_config, history = tscm.optimize(3
	,'RandomSearch'
	,optimization_kwargs = optimization_kwargs)
	\end{lstlisting}