% This file was created with Citavi 6.3.0.0

@proceedings{.1995,
	year = {1995}
}


@misc{PythonSoftwareFoundation.2020,
	author = {{Python Software Foundation}},
	year = {2020},
	title = {The Python Language Reference},
	url = {https://docs.python.org/3.7/reference/},
	urldate = {19.02.2020}
}


@misc{PSIORIGmbH.2020,
	author = {{PSIORI GmbH}},
	year = {2020},
	title = {PSIORI},
	url = {https://www.psiori.com/de},
	urldate = {19.02.2020}
}


@misc{PSIORIGmbH.2019,
	author = {{PSIORI GmbH}},
	date = {2019},
	title = {psipy documentation},
	url = {https://psipy.azurewebsites.net/source/psipy.html},
	urldate = {3.01.2020},
	editor = {{PSIORI GmbH}},
	institution = {{PSIORI GmbH}}
}


@misc{PSIORIGmbH.2019b,
	author = {{PSIORI GmbH}},
	date = {2019},
	title = {Autocrane wiki},
	url = {https://psiori.atlassian.net/wiki/spaces/2A/overview},
	editor = {{PSIORI GmbH}}
}


@misc{ProjectJupyter.,
	author = {{Project Jupyter}},
	title = {jupyter},
	url = {https://jupyter.org/}
}


@article{Pedregosa.2011,
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	year = {2011},
	title = {Scikit-learn: Machine Learning in Python},
	pages = {2825--2830},
	volume = {12},
	journal = {Journal of Machine Learning Research}
}


@misc{Oliphant.2006,
	author = {Oliphant, Travis},
	editor = {{Trelgol Publishing USA}},
	year = {2006},
	title = {NumPy: A guide to NumPy},
	url = {http://www.numpy.org/}
}


@misc{Nvidia.2020,
	author = {Nvidia},
	year = {2020},
	title = {Tesla K80},
	url = {https://www.nvidia.com/de-de/data-center/tesla-k80/},
	urldate = {19.02.2020}
}


@misc{Micorsoft.2020,
	author = {Micorsoft},
	year = {2020},
	title = {Microsoft Azure},
	url = {https://azure.microsoft.com/de-de/},
	urldate = {19.03.2020}
}


@incollection{Michelucci.2018,
	abstract = {In this chapter, you will look at the problem of finding the best hyperparameters to get the best results from your models. First, I will describe what a black-box optimization problem is, and how that class of problems relate to hyperparameter tuning. You will see the three best-known methods to tackle these kind of problems: grid search, random search, and Bayesian optimization. I will show you, with examples, which one works under which conditions, and I will give you a few tricks that are very helpful for improving optimization and sampling on a logarithmic scale. At the end of the chapter, I will show you how you can use those techniques to tune a deep model, using the Zalando dataset.},
	author = {Michelucci, Umberto},
	title = {Hyperparameter Tuning},
	pages = {271--322},
	publisher = {Apress},
	isbn = {978-1-4842-3790-8},
	booktitle = {Applied Deep Learning: A Case-Based Approach to Understanding Deep Neural Networks},
	year = {2018},
	address = {Berkeley, CA},
	doi = {10.1007/978-1-4842-3790-8$\backslash$textunderscore}
}


@inproceedings{Masci.2011,
	abstract = {We present a novel convolutional auto-encoder (CAE) for unsupervised feature learning. A stack of CAEs forms a convolutional neural network (CNN). Each CAE is trained using conventional on-line gradient descent without additional regularization terms. A max-pooling layer is essential to learn biologically plausible features consistent with those found by previous approaches. Initializing a CNN with filters of a trained CAE stack yields superior performance on a digit (MNIST) and an object recognition (CIFAR10) benchmark.},
	author = {Masci, Jonathan and Meier, Ueli and Cire{\c{s}}an, Dan and Schmidhuber, J{\"u}rgen},
	title = {Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction},
	pages = {52--59},
	publisher = {{Springer Berlin Heidelberg}},
	isbn = {978-3-642-21735-7},
	editor = {Honkela, Timo and Duch, W{\l}odzis{\l}aw and Girolami, Mark and Kaski, Samuel},
	booktitle = {Artificial Neural Networks and Machine Learning -- ICANN 2011},
	year = {2011},
	address = {Berlin, Heidelberg}
}


@misc{MartinAbadi.2015,
	author = {{Martin Abadi} and {Ashish Agarwal} and {Paul Barham} and {Eugene Brevdo} and {Zhifeng Chen} and {Craig Citro} and Corrado, Greg S and {Andy Davis} and {Jeffrey Dean} and {Matthieu Devin} and {Sanjay Ghemawat} and {Ian Goodfellow} and {Andrew Harp} and {Geoffrey Irving} and {Michael Isard} and Jia, Yangqing and {Rafal Jozefowicz} and {Lukasz Kaiser} and {Manjunath Kudlur} and {Josh Levenberg} and {Dandelion Man{\'e}} and {Rajat Monga} and {Sherry Moore} and {Derek Murray} and {Chris Olah} and {Mike Schuster} and {Jonathon Shlens} and {Benoit Steiner} and {Ilya Sutskever} and {Kunal Talwar} and {Paul Tucker} and {Vincent Vanhoucke} and {Vijay Vasudevan} and {Fernanda Vi{\'e}gas} and {Oriol Vinyals} and {Pete Warden} and {Martin Wattenberg} and {Martin Wicke} and {Yuan Yu} and {Xiaoqiang Zheng}},
	year = {2015},
	title = {TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems},
	url = {https://www.tensorflow.org/}
}


@article{Lindauer.,
	author = {Lindauer, M. and Eggensperger, K. and Feurer, M. and Biedenkapp, A. and Marben, J. and M{\"u}ller, P. and Hutter, F.},
	title = {BOAH: A Tool Suite for Multi-Fidelity Bayesian Optimization {\&} Analysis of Hyperparameters},
	journal = {arXiv:1908.06756  In Citavi anzeigen[cs.LG]}
}


@article{Li.2017,
	author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
	year = {2017},
	title = {HYPERBAND: BANDIT-BASED CONFIGURATION EVALUATION FOR HYPERPARAMETER OPTIMIZATION},
	url = {https://openreview.net/pdf?id=ry18Ww5ee},
	journal = {ICLR},
	file = {hyperband_bandit_based_configuration_evaluation_for_hyperparameter_optimization.pdf}
}


@incollection{LeCun.1999,
	abstract = {Finding an appropriate set of features is an essential problem in the design of shape recognition systems. This paper attempts to show that for recognizing simple objects with high shape variability such as handwritten characters, it is possible, and even advantageous, to feed the system directly with minimally processed images and to rely on learning to extract the right set of features. Convolutional Neural Networks are shown to be particularly well suited to this task. We also show that these networks can be used to recognize multiple objects without requiring explicit segmentation of the objects from their surrounding. The second part of the paper presents the Graph Transformer Network model which extends the applicability of gradient-based learning to systems that use graphs to represents features, objects, and their combinations.},
	author = {LeCun, Yann and Haffner, Patrick and Bottou, L{\'e}on and Bengio, Yoshua},
	title = {Object Recognition with Gradient-Based Learning},
	pages = {319--345},
	publisher = {{Springer Berlin Heidelberg}},
	isbn = {978-3-540-46805-9},
	booktitle = {Shape, Contour and Grouping in Computer Vision},
	year = {1999},
	address = {Berlin, Heidelberg},
	doi = {10.1007/3-540-46805-6{\textunderscore }19}
}


@incollection{Kohavi.,
	author = {Kohavi, Ron and John, George},
	title = {Autmatic Parameter Selection by Minimizing Estimated Error},
	pages = {304--312}
}


@article{JoaquinVanschoren.2018,
	author = {{Joaquin Vanschoren}},
	year = {2018},
	title = {Meta-Learning: A Survey},
	volume = {abs/1810.03548},
	journal = {CoRR},
	file = {http://arxiv.org/abs/1810.03548}
}


@book{.1999,
	year = {1999},
	title = {Shape, Contour and Grouping in Computer Vision},
	address = {Berlin, Heidelberg},
	publisher = {{Springer Berlin Heidelberg}},
	isbn = {978-3-540-46805-9}
}


@proceedings{.2007,
	year = {2007}
}


@book{.2018,
	year = {2018},
	title = {Applied Deep Learning: A Case-Based Approach to Understanding Deep Neural Networks},
	address = {Berkeley, CA},
	publisher = {Apress},
	isbn = {978-1-4842-3790-8}
}


@inproceedings{Bengio.2007,
	author = {Bengio, Y. and Lamblin, Pascal and Popovici, D. and Larochelle, Hugo and Montreal, U.},
	title = {Greedy layer-wise training of deep networks},
	volume = {19},
	year = {2007}
}


@article{BergstraJamesandYoshuaBengio..2012,
	author = {{Bergstra, James, and Yoshua Bengio.}},
	year = {2012},
	title = {Random search for hyper-parameter optimization.},
	url = {http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf},
	pages = {281--305},
	journal = {Journal of Machine Learning Research 13},
	file = {Random_Search_bergstra12a.pdf}
}


@misc{Chollet.2015,
	author = {Chollet, Fran{\c{c}}ois and others},
	year = {2015},
	title = {Keras}
}


@article{StefanFalkner.2018,
	abstract = {Proceedings of the International Conference on Machine Learning 2018},
	author = {{Stefan Falkner} and {Aaron Klein} and {Frank Hutter}},
	year = {2018},
	title = {BOHB: Robust and Efficient Hyperparameter Optimization at Scale},
	url = {https://openreview.net/pdf?id=ry18Ww5ee},
	keywords = {ICML;Machine Learning},
	journal = {arXiv:1807.01774},
	file = {18-ICML-BOHB.pdf}
}


@misc{cnvrg.io.,
	author = {cnvrg.io},
	title = {cnvrg.io},
	url = {https://cnvrg.io/},
	urldate = {19.02.2020}
}


@incollection{Elsken.2019,
	author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
	title = {Neural Architecture Search: 3},
	pages = {69--86},
	publisher = {Springer},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	booktitle = {Automatic Machine Learning: Methods, Systems, Challenges},
	year = {2019}
}


@incollection{Feurer.2019,
	author = {Feurer, Matthias and Hutter, Frank},
	title = {Hyperparameter Optimization: 1},
	pages = {3--38},
	publisher = {Springer},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	booktitle = {Automatic Machine Learning: Methods, Systems, Challenges},
	year = {2019}
}


@article{Hinton.2006,
	author = {Hinton, G. E. and Salakhutdinov, R. R.},
	year = {2006},
	title = {Reducing the Dimensionality of Data with Neural Networks},
	pages = {504--507},
	volume = {313},
	journal = {Science (New York, N.Y.)},
	doi = {10.1126/science.1127647}
}


@proceedings{Honkela.2011,
	year = {2011},
	title = {Artificial Neural Networks and Machine Learning -- ICANN 2011},
	address = {Berlin, Heidelberg},
	publisher = {{Springer Berlin Heidelberg}},
	isbn = {978-3-642-21735-7},
	editor = {Honkela, Timo and Duch, W{\l}odzis{\l}aw and Girolami, Mark and Kaski, Samuel}
}


@misc{Hunter.2007,
	abstract = {Matplotlib is a 2D graphics package used for Python for
	
	application development, interactive scripting, and publication-quality
	
	image generation across user interfaces and operating systems.},
	author = {Hunter},
	year = {2007},
	title = {Matplotlib: A 2D graphics environment},
	volume = {9},
	number = {3},
	publisher = {{IEEE COMPUTER SOC}},
	journal = {Computing in Science {\&} Engineering},
	doi = {10.1109/MCSE.2007.55}
}


@book{Hutter.2019,
	year = {2019},
	title = {Automatic Machine Learning: Methods, Systems, Challenges},
	publisher = {Springer},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin}
}

% This file was created with Citavi 6.3.0.0

@book{.1987,
	year = {1987},
	title = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations}
}


@incollection{D.E.Rumelhart.1987,
	author = {{D. E. Rumelhart} and {J. L. McClelland}},
	title = {Learning Internal Representations by Error Propagation},
	pages = {318--362},
	booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations},
	year = {1987}
}




@article{Cohen.,
	abstract = {The MNIST dataset has become a standard benchmark for learning, classification and computer vision systems. Contributing to its widespread adoption are the understandable and intuitive nature of the task, its relatively small size and storage requirements and the accessibility and ease-of-use of the database itself. The MNIST database was derived from a larger dataset known as the NIST Special Database 19 which contains digits, uppercase and lowercase handwritten letters. This paper introduces a variant of the full NIST dataset, which we have called Extended MNIST (EMNIST), which follows the same conversion paradigm used to create the MNIST dataset. The result is a set of datasets that constitute a more challenging classification tasks involving letters and digits, and that shares the same image structure and parameters as the original MNIST task, allowing for direct compatibility with all existing classifiers and systems. Benchmark results are presented along with a validation of the conversion process through the comparison of the classification results on converted NIST digits and the MNIST digits.},
	author = {Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and {van Schaik}, Andr{\'e}},
	title = {EMNIST: an extension of MNIST to handwritten letters},
	url = {http://arxiv.org/pdf/1702.05373v2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {EMNIST_1702.05373v2.pdf},
	file = {http://arxiv.org/abs/1702.05373v2},
	file = {https://arxiv.org/pdf/1702.05373v2.pdf}
}


@incollection{Vanschoren.2019,
	author = {Vanschoren, Joaquin},
	title = {Meta-Learning: 2},
	pages = {39--68},
	publisher = {Springer},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	booktitle = {Automatic Machine Learning: Methods, Systems, Challenges},
	year = {2019}
}


