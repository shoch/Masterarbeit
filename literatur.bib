% This file was created with Citavi 6.4.0.35

@book{.1987,
	year = {1987},
	title = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations}
}


@article{LeCun.2015,
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	year = {2015},
	title = {Deep learning},
	pages = {436--444},
	volume = {521},
	number = {7553},
	issn = {1476-4687},
	journal = {Nature},
	doi = {10.1038/nature14539}
}


@incollection{LeCun.1999,
	abstract = {Finding an appropriate set of features is an essential problem in the design of shape recognition systems. This paper attempts to show that for recognizing simple objects with high shape variability such as handwritten characters, it is possible, and even advantageous, to feed the system directly with minimally processed images and to rely on learning to extract the right set of features. Convolutional Neural Networks are shown to be particularly well suited to this task. We also show that these networks can be used to recognize multiple objects without requiring explicit segmentation of the objects from their surrounding. The second part of the paper presents the Graph Transformer Network model which extends the applicability of gradient-based learning to systems that use graphs to represents features, objects, and their combinations.},
	author = {LeCun, Yann and Haffner, Patrick and Bottou, L{\'e}on and Bengio, Yoshua},
	title = {Object Recognition with Gradient-Based Learning},
	pages = {319--345},
	publisher = {{Springer Berlin Heidelberg}},
	isbn = {978-3-540-46805-9},
	booktitle = {Shape, Contour and Grouping in Computer Vision},
	year = {1999},
	address = {Berlin, Heidelberg},
	doi = {10.1007/3-540-46805-6{\textunderscore }19}
}


@article{Li.2017,
	author = {Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
	year = {2017},
	title = {HYPERBAND: BANDIT-BASED CONFIGURATION EVALUATION FOR HYPERPARAMETER OPTIMIZATION},
	url = {https://openreview.net/pdf?id=ry18Ww5ee},
	journal = {ICLR},
	file = {hyperband_bandit_based_configuration_evaluation_for_hyperparameter_optimization.pdf}
}


@article{Li.2016,
	author = {Li, Xi and Zhao, Liming and Wei, Lina and Yang, Ming-Hsuan and Wu, Fei and Zhuang, Yueting and Ling, Haibin and Wang, Jingdong},
	year = {2016},
	title = {DeepSaliency: Multi-Task Deep Neural Network Model for Salient Object Detection},
	pages = {3919--3930},
	volume = {25},
	number = {8},
	issn = {1941-0042},
	journal = {IEEE Transactions on Image Processing},
	doi = {10.1109/tip.2016.2579306}
}


@misc{Lindauer.2019,
	abstract = {Hyperparameter optimization and neural architecture search can become prohibitively expensive for regular black-box Bayesian optimization because the training and evaluation of a single model can easily take several hours. To overcome this, we introduce a comprehensive tool suite for effective multi-fidelity Bayesian optimization and the analysis of its runs. The suite, written in Python, provides a simple way to specify complex design spaces, a robust and efficient combination of Bayesian optimization and HyperBand, and a comprehensive analysis of the optimization process and its outcomes.},
	author = {Lindauer, Marius and Eggensperger, Katharina and Feurer, Matthias and Biedenkapp, Andr{\'e} and Marben, Joshua and M{\"u}ller, Philipp and Hutter, Frank},
	date = {2019},
	title = {BOAH: A Tool Suite for Multi-Fidelity Bayesian Optimization {\&} Analysis  of Hyperparameters},
	url = {https://arxiv.org/pdf/1908.06756},
	file = {https://arxiv.org/pdf/1908.06756v1.pdf}
}


@misc{Liu.2015,
	abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For {\$}300$\backslash$times 300{\$} input, SSD achieves 72.1{\%} mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for {\$}500$\backslash$times 500{\$} input, SSD achieves 75.1{\%} mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at https://github.com/weiliu89/caffe/tree/ssd .},
	author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
	date = {2016},
	title = {SSD: Single Shot MultiBox Detector},
	url = {https://arxiv.org/pdf/1512.02325},
	doi = {10.1007/978-3-319-46448-0{\textunderscore }2},
	file = {https://arxiv.org/pdf/1512.02325v5.pdf}
}


@incollection{Long.2016,
	author = {Long, Mingsheng and Zhu, Han and Wang, Jianmin and Jordan, Michael I.},
	title = {Unsupervised Domain Adaptation with Residual Transfer Networks},
	url = {http://papers.nips.cc/paper/6110-unsupervised-domain-adaptation-with-residual-transfer-networks.pdf},
	pages = {136--144},
	publisher = {{Curran Associates, Inc}},
	editor = {{D. D. Lee} and {M. Sugiyama} and {U. V. Luxburg} and {I. Guyon} and {R. Garnett}},
	booktitle = {Advances in Neural Information Processing Systems 29},
	year = {2016}
}


@misc{MartinAbadi.2015,
	author = {{Martin Abadi} and {Ashish Agarwal} and {Paul Barham} and {Eugene Brevdo} and {Zhifeng Chen} and {Craig Citro} and Corrado, Greg S and {Andy Davis} and {Jeffrey Dean} and {Matthieu Devin} and {Sanjay Ghemawat} and {Ian Goodfellow} and {Andrew Harp} and {Geoffrey Irving} and {Michael Isard} and Jia, Yangqing and {Rafal Jozefowicz} and {Lukasz Kaiser} and {Manjunath Kudlur} and {Josh Levenberg} and {Dandelion Man{\'e}} and {Rajat Monga} and {Sherry Moore} and {Derek Murray} and {Chris Olah} and {Mike Schuster} and {Jonathon Shlens} and {Benoit Steiner} and {Ilya Sutskever} and {Kunal Talwar} and {Paul Tucker} and {Vincent Vanhoucke} and {Vijay Vasudevan} and {Fernanda Vi{\'e}gas} and {Oriol Vinyals} and {Pete Warden} and {Martin Wattenberg} and {Martin Wicke} and {Yuan Yu} and {Xiaoqiang Zheng}},
	year = {2015},
	title = {TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems},
	url = {https://www.tensorflow.org/}
}


@inproceedings{Masci.2011,
	abstract = {We present a novel convolutional auto-encoder (CAE) for unsupervised feature learning. A stack of CAEs forms a convolutional neural network (CNN). Each CAE is trained using conventional on-line gradient descent without additional regularization terms. A max-pooling layer is essential to learn biologically plausible features consistent with those found by previous approaches. Initializing a CNN with filters of a trained CAE stack yields superior performance on a digit (MNIST) and an object recognition (CIFAR10) benchmark.},
	author = {Masci, Jonathan and Meier, Ueli and Cire{\c{s}}an, Dan and Schmidhuber, J{\"u}rgen},
	title = {Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction},
	pages = {52--59},
	publisher = {{Springer Berlin Heidelberg}},
	isbn = {978-3-642-21735-7},
	editor = {Honkela, Timo and Duch, W{\l}odzis{\l}aw and Girolami, Mark and Kaski, Samuel},
	booktitle = {Artificial Neural Networks and Machine Learning -- ICANN 2011},
	year = {2011},
	address = {Berlin, Heidelberg}
}


@incollection{Michelucci.2018,
	abstract = {In this chapter, you will look at the problem of finding the best hyperparameters to get the best results from your models. First, I will describe what a black-box optimization problem is, and how that class of problems relate to hyperparameter tuning. You will see the three best-known methods to tackle these kind of problems: grid search, random search, and Bayesian optimization. I will show you, with examples, which one works under which conditions, and I will give you a few tricks that are very helpful for improving optimization and sampling on a logarithmic scale. At the end of the chapter, I will show you how you can use those techniques to tune a deep model, using the Zalando dataset.},
	author = {Michelucci, Umberto},
	title = {Hyperparameter Tuning},
	pages = {271--322},
	publisher = {Apress},
	isbn = {978-1-4842-3790-8},
	booktitle = {Applied Deep Learning: A Case-Based Approach to Understanding Deep Neural Networks},
	year = {2018},
	address = {Berkeley, CA},
	doi = {10.1007/978-1-4842-3790-8$\backslash$textunderscore}
}


@misc{Micorsoft.2020,
	author = {Micorsoft},
	year = {2020},
	title = {Microsoft Azure},
	url = {https://azure.microsoft.com/de-de/},
	urldate = {2020-03-19}
}


@misc{Nvidia.2020,
	author = {Nvidia},
	year = {2020},
	title = {Tesla K80},
	url = {https://www.nvidia.com/de-de/data-center/tesla-k80/},
	urldate = {2020-02-19}
}


@misc{Oliphant.2006,
	author = {Oliphant, Travis},
	editor = {{Trelgol Publishing USA}},
	year = {2006},
	title = {NumPy: A guide to NumPy},
	url = {http://www.numpy.org/}
}


@article{Pan.2010,
	author = {Pan, Sinno Jialin and Qiang, Yang},
	year = {2010},
	title = {A Survey on Transfer Learning},
	pages = {1345--1359},
	volume = {22},
	number = {10},
	journal = {IEEE Transactions on Knowledge and Data Engineering}
}


@proceedings{Kurkova.2018,
	year = {2018},
	title = {Artificial Neural Networks and Machine Learning -- ICANN 2018},
	address = {Cham},
	publisher = {{Springer International Publishing}},
	isbn = {978-3-030-01424-7},
	editor = {K{\r{u}}rkov{\'a}, V{\v{e}}ra and Manolopoulos, Yannis and Hammer, Barbara and Iliadis, Lazaros and Maglogiannis, Ilias}
}


@article{Pedregosa.2011,
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	year = {2011},
	title = {Scikit-learn: Machine Learning in Python},
	pages = {2825--2830},
	volume = {12},
	journal = {Journal of Machine Learning Research}
}


@misc{PSIORIGmbH.2019,
	author = {{PSIORI GmbH}},
	date = {2019},
	title = {Autocrane wiki},
	url = {https://psiori.atlassian.net/wiki/spaces/2A/overview},
	editor = {{PSIORI GmbH}}
}


@misc{PSIORIGmbH.2019b,
	author = {{PSIORI GmbH}},
	date = {2019},
	title = {psipy documentation},
	url = {https://psipy.azurewebsites.net/source/psipy.html},
	urldate = {2020-01-03},
	editor = {{PSIORI GmbH}},
	institution = {{PSIORI GmbH}}
}


@misc{PSIORIGmbH.2020,
	author = {{PSIORI GmbH}},
	year = {2020},
	title = {PSIORI},
	url = {https://www.psiori.com/de},
	urldate = {2020-02-19}
}


@misc{PythonSoftwareFoundation.2020,
	author = {{Python Software Foundation}},
	year = {2020},
	title = {The Python Language Reference},
	url = {https://docs.python.org/3.7/reference/},
	urldate = {2020-02-19}
}


@misc{RajeevRanjan.2016,
	author = {{Rajeev Ranjan} and {Vishal M. Patel} and {Rama Chellappa}},
	year = {2016},
	title = {HyperFace: A Deep Multi-task Learning Framework for Face Detection, Landmark Localization, Pose Estimation, and Gender Recognition},
	url = {https://arxiv.org/abs/1603.01249}
}


@inproceedings{Rifai.2011,
	author = {Rifai, Salah and Vincent, Pascal and Muller, Xavier and Glorot, Xavier and Bengio, Yoshua},
	title = {Contractive Auto-Encoders: Explicit Invariance during Feature Extraction},
	pages = {833--840},
	publisher = {Omnipress},
	isbn = {9781450306195},
	series = {ICML'11},
	booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
	year = {2011},
	address = {Madison, WI, USA}
}


@article{Shearer.2000,
	author = {Shearer, Colin},
	year = {2000},
	title = {The CRISP-DM Model: The New Blueprint for Data Mining},
	keywords = {imported},
	volume = {5},
	number = {4},
	journal = {Journal of Data Warehousing}
}


@article{StefanFalkner.2018,
	abstract = {Proceedings of the International Conference on Machine Learning 2018},
	author = {{Stefan Falkner} and {Aaron Klein} and {Frank Hutter}},
	year = {2018},
	title = {BOHB: Robust and Efficient Hyperparameter Optimization at Scale},
	url = {https://openreview.net/pdf?id=ry18Ww5ee},
	keywords = {ICML;Machine Learning},
	journal = {arXiv:1807.01774},
	file = {18-ICML-BOHB.pdf}
}


@inproceedings{Tan.2018,
	abstract = {As a new classification platform, deep learning has recently received increasing attention from researchers and has been successfully applied to many domains. In some domains, like bioinformatics and robotics, it is very difficult to construct a large-scale well-annotated dataset due to the expense of data acquisition and costly annotation, which limits its development. Transfer learning relaxes the hypothesis that the training data must be independent and identically distributed (i.i.d.) with the test data, which motivates us to use transfer learning to solve the problem of insufficient training data. This survey focuses on reviewing the current researches of transfer learning by using deep neural network and its applications. We defined deep transfer learning, category and review the recent research works based on the techniques used in deep transfer learning.},
	author = {Tan, Chuanqi and Sun, Fuchun and Kong, Tao and Zhang, Wenchang and Yang, Chao and Liu, Chunfang},
	title = {A Survey on Deep Transfer Learning},
	pages = {270--279},
	publisher = {{Springer International Publishing}},
	isbn = {978-3-030-01424-7},
	editor = {K{\r{u}}rkov{\'a}, V{\v{e}}ra and Manolopoulos, Yannis and Hammer, Barbara and Iliadis, Lazaros and Maglogiannis, Ilias},
	booktitle = {Artificial Neural Networks and Machine Learning -- ICANN 2018},
	year = {2018},
	address = {Cham}
}


@book{Thrun.1998,
	year = {1998},
	title = {Learning to Learn},
	address = {Boston, MA},
	publisher = {{Springer US}},
	isbn = {978-1-4615-5529-2},
	editor = {Thrun, Sebastian and Pratt, Lorien}
}


@article{Thung.2018,
	author = {Thung, Kimhan and Wee, Chong-Yaw},
	year = {2018},
	title = {A brief review on multi-task learning},
	volume = {77},
	journal = {Multimedia Tools and Applications},
	doi = {10.1007/s11042-018-6463-x}
}


@inproceedings{Tirumala.2018,
	abstract = {Deep Transfer Learning or DTS has proven successful with deep neural networks and deep belief networks. However, there has been limited research on to using deep autoencoder (DAE)-based network to implement DTS. This paper for the first time attempts to identify transferable features in the form of learning and transfer them to another network implementing a simple DTS mechanism. In this paper, a transfer of knowledge process is proposed where in knowledge is transferred from one Deep autoencoder network to another. This knowledge transfer has helped to improve the classification accuracy of the receiving autoencoder, particularly when experimented using corrupted dataset. The experiments are carried out on a texa based hierarchical dataset. Firstly, a DAE is trained with regular undamaged dataset to achieve maximum accuracy. Then, a distorted dataset was used to train second DAEN for classification with which only 56.7{\%} of the data is correctly classified. Then a set of weights are transferred from from first DAEN to the second DAEN which resulted in an an improvement of classification accuracy by about 22{\%}. The key contribution of this paper is highlighting importance of knowledge transfer between two deep autoencoder networks which is proposed for the first time.},
	author = {Tirumala, Sreenivas Sremath},
	title = {A Deep Autoencoder-Based Knowledge Transfer Approach},
	pages = {277--284},
	publisher = {{Springer Singapore}},
	isbn = {978-981-10-6319-0},
	editor = {Chaki, Nabendu and Cortesi, Agostino and Devarakonda, Nagaraju},
	booktitle = {Proceedings of International Conference on Computational Intelligence and Data Engineering},
	year = {2018},
	address = {Singapore}
}


@incollection{Vanschoren.2019,
	author = {Vanschoren, Joaquin},
	title = {Meta-Learning: 2},
	pages = {39--68},
	publisher = {Springer},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	booktitle = {Automatic Machine Learning: Methods, Systems, Challenges},
	year = {2019}
}


@inproceedings{Vincent.2008,
	author = {Vincent, Pascal and Larochelle, Hugo and Bengio, Y. and Manzagol, Pierre-Antoine},
	title = {Extracting and composing robust features with denoising autoencoders},
	pages = {1096--1103},
	year = {2008},
	doi = {10.1145/1390156.1390294}
}


@misc{ProjectJupyter.,
	author = {{Project Jupyter}},
	title = {jupyter},
	url = {https://jupyter.org/}
}


@article{Krizhevsky.2012,
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey},
	year = {2012},
	title = {ImageNet Classification with Deep Convolutional Neural Networks},
	volume = {25},
	journal = {Neural Information Processing Systems},
	doi = {10.1145/3065386}
}


@inproceedings{Kohavi.1995,
	author = {Kohavi, Ron and John, George H.},
	title = {Automatic Parameter Selection by Minimizing Estimated Error},
	pages = {304--312},
	publisher = {{Morgan Kaufmann}},
	booktitle = {In Proceedings of the Twelfth International Conference on Machine Learning},
	year = {1995}
}


@article{Kingma.2019,
	author = {Kingma, Diederik P. and Welling, Max},
	year = {2019},
	title = {An Introduction to Variational Autoencoders},
	url = {http://dx.doi.org/10.1561/2200000056},
	pages = {307--392},
	volume = {12},
	number = {4},
	issn = {1935-8245},
	journal = {Foundations and Trends{\circledR} in Machine Learning},
	doi = {10.1561/2200000056}
}


@proceedings{.1995,
	year = {1995},
	title = {In Proceedings of the Twelfth International Conference on Machine Learning},
	publisher = {{Morgan Kaufmann}}
}


@book{.1999,
	year = {1999},
	title = {Shape, Contour and Grouping in Computer Vision},
	address = {Berlin, Heidelberg},
	publisher = {{Springer Berlin Heidelberg}},
	isbn = {978-3-540-46805-9}
}


@proceedings{.2007,
	year = {2007}
}


@proceedings{.2008,
	year = {2008}
}


@proceedings{.2011,
	year = {2011},
	title = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
	address = {Madison, WI, USA},
	publisher = {Omnipress},
	isbn = {9781450306195},
	series = {ICML'11}
}


@proceedings{.2017,
	year = {2017},
	title = {Proceedings of the Twenty-Sixth International Joint Conference on  Artificial Intelligence, IJCAI-17}
}


@book{.2018,
	year = {2018},
	title = {Applied Deep Learning: A Case-Based Approach to Understanding Deep Neural Networks},
	address = {Berkeley, CA},
	publisher = {Apress},
	isbn = {978-1-4842-3790-8}
}


@inproceedings{Bengio.2007,
	author = {Bengio, Y. and Lamblin, Pascal and Popovici, D. and Larochelle, Hugo and Montreal, U.},
	title = {Greedy layer-wise training of deep networks},
	volume = {19},
	year = {2007}
}


@article{BergstraJamesandYoshuaBengio..2012,
	author = {{Bergstra, James, and Yoshua Bengio.}},
	year = {2012},
	title = {Random search for hyper-parameter optimization.},
	url = {http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf},
	pages = {281--305},
	journal = {Journal of Machine Learning Research 13},
	file = {Random_Search_bergstra12a.pdf}
}


@incollection{Caruana.1998,
	abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
	author = {Caruana, Rich},
	title = {Multitask Learning},
	pages = {95--133},
	publisher = {{Springer US}},
	isbn = {978-1-4615-5529-2},
	editor = {Thrun, Sebastian and Pratt, Lorien},
	booktitle = {Learning to Learn},
	year = {1998},
	address = {Boston, MA},
	doi = {10.1007/978-1-4615-5529-2{\textunderscore }5}
}


@proceedings{Chaki.2018,
	year = {2018},
	title = {Proceedings of International Conference on Computational Intelligence and Data Engineering},
	address = {Singapore},
	publisher = {{Springer Singapore}},
	isbn = {978-981-10-6319-0},
	editor = {Chaki, Nabendu and Cortesi, Agostino and Devarakonda, Nagaraju}
}


@book{Chapelle.2010,
	author = {Chapelle, Olivier and Schlkopf, Bernhard and Zien, Alexander},
	year = {2010},
	title = {Semi-Supervised Learning},
	edition = {1st},
	publisher = {{The MIT Press}},
	isbn = {0262514125}
}


@misc{Chollet.2015,
	author = {Chollet, Fran{\c{c}}ois and others},
	year = {2015},
	title = {Keras},
	url = {https://keras.io/}
}


@misc{ChristianSzegedy.2014,
	author = {{Christian Szegedy} and {Wei Liu} and {Yangqing Jia} and {Pierre Sermanet} and {Scott Reed} and {Dragomir Anguelov} and {Dumitru Erhan} and {Vincent Vanhoucke} and {Andrew Rabinovich}},
	year = {2014},
	title = {Going Deeper with Convolutions},
	url = {https://arxiv.org/abs/1409.4842}
}


@misc{cnvrg.io.2020,
	author = {cnvrg.io},
	year = {2020},
	title = {cnvrg},
	url = {https://cnvrg.io/},
	urldate = {2020-02-19},
	publisher = {cnvrg.io}
}


@article{Cohen.,
	abstract = {The MNIST dataset has become a standard benchmark for learning, classification and computer vision systems. Contributing to its widespread adoption are the understandable and intuitive nature of the task, its relatively small size and storage requirements and the accessibility and ease-of-use of the database itself. The MNIST database was derived from a larger dataset known as the NIST Special Database 19 which contains digits, uppercase and lowercase handwritten letters. This paper introduces a variant of the full NIST dataset, which we have called Extended MNIST (EMNIST), which follows the same conversion paradigm used to create the MNIST dataset. The result is a set of datasets that constitute a more challenging classification tasks involving letters and digits, and that shares the same image structure and parameters as the original MNIST task, allowing for direct compatibility with all existing classifiers and systems. Benchmark results are presented along with a validation of the conversion process through the comparison of the classification results on converted NIST digits and the MNIST digits.},
	author = {Cohen, Gregory and Afshar, Saeed and Tapson, Jonathan and {van Schaik}, Andr{\'e}},
	title = {EMNIST: an extension of MNIST to handwritten letters},
	url = {http://arxiv.org/pdf/1702.05373v2},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {EMNIST_1702.05373v2.pdf},
	file = {http://arxiv.org/abs/1702.05373v2},
	file = {https://arxiv.org/pdf/1702.05373v2.pdf}
}


@book{D.D.Lee.2016,
	year = {2016},
	title = {Advances in Neural Information Processing Systems 29},
	publisher = {{Curran Associates, Inc}},
	editor = {{D. D. Lee} and {M. Sugiyama} and {U. V. Luxburg} and {I. Guyon} and {R. Garnett}}
}


@article{JoaquinVanschoren.2018,
	author = {{Joaquin Vanschoren}},
	year = {2018},
	title = {Meta-Learning: A Survey},
	volume = {abs/1810.03548},
	journal = {CoRR},
	file = {http://arxiv.org/abs/1810.03548}
}


@misc{JasonYosinski.2014,
	author = {{Jason Yosinski} and {Jeff Clune} and {Yoshua Bengio} and {Hod Lipson}},
	year = {2014},
	title = {How transferable are features in deep neural networks?},
	url = {https://arxiv.org/abs/1411.1792}
}


@misc{Jamieson.2015,
	abstract = {Motivated by the task of hyperparameter optimization, we introduce the non-stochastic best-arm identification problem. Within the multi-armed bandit literature, the cumulative regret objective enjoys algorithms and analyses for both the non-stochastic and stochastic settings while to the best of our knowledge, the best-arm identification framework has only been considered in the stochastic setting. We introduce the non-stochastic setting under this framework, identify a known algorithm that is well-suited for this setting, and analyze its behavior. Next, by leveraging the iterative nature of standard machine learning algorithms, we cast hyperparameter optimization as an instance of non-stochastic best-arm identification, and empirically evaluate our proposed algorithm on this task. Our empirical results show that, by allocating more resources to promising hyperparameter settings, we typically achieve comparable test accuracies an order of magnitude faster than baseline methods.},
	author = {Jamieson, Kevin and Talwalkar, Ameet},
	date = {2015},
	title = {Non-stochastic Best Arm Identification and Hyperparameter Optimization},
	url = {https://arxiv.org/pdf/1502.07943},
	file = {https://arxiv.org/pdf/1502.07943v1.pdf}
}


@misc{IanJ.Goodfellow.2014,
	author = {{Ian J. Goodfellow} and {Jean Pouget-Abadie} and {Mehdi Mirza} and {Bing Xu} and {David Warde-Farley} and {Sherjil Ozair} and {Aaron Courville} and {Yoshua Bengio}},
	year = {2014},
	title = {Generative Adversarial Networks}
}


@book{Hutter.2019,
	year = {2019},
	title = {Automatic Machine Learning: Methods, Systems, Challenges},
	publisher = {Springer},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin}
}


@misc{Hunter.2007,
	abstract = {Matplotlib is a 2D graphics package used for Python for
	
	application development, interactive scripting, and publication-quality
	
	image generation across user interfaces and operating systems.},
	author = {Hunter},
	year = {2007},
	title = {Matplotlib: A 2D graphics environment},
	volume = {9},
	number = {3},
	publisher = {{IEEE COMPUTER SOC}},
	journal = {Computing in Science {\&} Engineering},
	doi = {10.1109/MCSE.2007.55}
}


@inproceedings{YuchunFang.2017,
	author = {{Yuchun Fang} and {Zhengyan Ma} and {Zhaoxiang Zhang} and {Xu-Yao Zhang} and {Xiang Bai}},
	title = {Dynamic Multi-Task Learning with Convolutional Neural Network},
	pages = {1668--1674},
	booktitle = {Proceedings of the Twenty-Sixth International Joint Conference on  Artificial Intelligence, IJCAI-17},
	year = {2017},
	doi = {10.24963/ijcai.2017/231}
}


@proceedings{Honkela.2011,
	year = {2011},
	title = {Artificial Neural Networks and Machine Learning -- ICANN 2011},
	address = {Berlin, Heidelberg},
	publisher = {{Springer Berlin Heidelberg}},
	isbn = {978-3-642-21735-7},
	editor = {Honkela, Timo and Duch, W{\l}odzis{\l}aw and Girolami, Mark and Kaski, Samuel}
}


@article{George.2018,
	author = {George, Daniel and Shen, Hongyu and Huerta, E. A.},
	year = {2018},
	title = {Classification and unsupervised clustering of LIGO data with Deep Transfer Learning},
	volume = {97},
	number = {10},
	issn = {2470-0029},
	journal = {Physical Review D},
	doi = {10.1103/physrevd.97.101501}
}


@misc{FuzhenZhuang.2019,
	author = {{Fuzhen Zhuang} and {Zhiyuan Qi} and {Keyu Duan} and {Dongbo Xi} and {Yongchun Zhu} and {Hengshu Zhu} and {Hui Xiong} and {Qing He}},
	title = {A Comprehensive Survey on Transfer Learning},
	url = {https://arxiv.org/abs/1911.02685v2},
	urldate = {2020-04-01}
}


@misc{Frazier.201807,
	abstract = {Bayesian optimization is an approach to optimizing objective functions that take a long time (minutes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the field. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.},
	author = {Frazier, Peter I.},
	date = {2018-07},
	title = {A Tutorial on Bayesian Optimization},
	url = {https://arxiv.org/pdf/1807.02811},
	file = {https://arxiv.org/pdf/1807.02811v1.pdf}
}


@incollection{Feurer.2019,
	author = {Feurer, Matthias and Hutter, Frank},
	title = {Hyperparameter Optimization: 1},
	pages = {3--38},
	publisher = {Springer},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	booktitle = {Automatic Machine Learning: Methods, Systems, Challenges},
	year = {2019}
}


@incollection{Elsken.2019,
	author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
	title = {Neural Architecture Search: 3},
	pages = {69--86},
	publisher = {Springer},
	editor = {Hutter, Frank and Kotthoff, Lars and Vanschoren, Joaquin},
	booktitle = {Automatic Machine Learning: Methods, Systems, Challenges},
	year = {2019}
}


@incollection{D.E.Rumelhart.1987,
	author = {{D. E. Rumelhart} and {J. L. McClelland}},
	title = {Learning Internal Representations by Error Propagation},
	pages = {318--362},
	booktitle = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition: Foundations},
	year = {1987}
}


@article{Hinton.2006,
	author = {Hinton, G. E. and Salakhutdinov, R. R.},
	year = {2006},
	title = {Reducing the Dimensionality of Data with Neural Networks},
	pages = {504--507},
	volume = {313},
	journal = {Science (New York, N.Y.)},
	doi = {10.1126/science.1127647}
}


@article{Zhao.2019,
	abstract = {Face alignment and segmentation are challenging problems which have been extensively studied in the field of multimedia. These two tasks are closely related and their learning processes are supposed to benefit each other. Hence, we present a joint multi-task learning algorithm for both face alignment and segmentation using deep convolutional neural network (CNN). The proposed multi-task learning approach allows CNN model to simultaneously share visual knowledge between different tasks. With a carefully designed refinement residual module, the cross-layer features are fused in a collaborative manner. To the best of our knowledge, this is the first time that face alignment and segmentation are learned together via deep multi-task learning. Our experiments show that learning these two related tasks simultaneously builds a synergy between them, improves the performance of each individual task, and rivals recent approaches. Furthermore, we demonstrate the effectiveness of our model in two practical applications: virtual makeup and face swap.},
	author = {Zhao, Yucheng and Tang, Fan and Dong, Weiming and Huang, Feiyue and Zhang, Xiaopeng},
	year = {2019},
	title = {Joint face alignment and segmentation via deep multi-task learning},
	pages = {13131--13148},
	volume = {78},
	number = {10},
	journal = {Multimedia Tools and Applications},
	doi = {10.1007/s11042-018-5609-1}
}


